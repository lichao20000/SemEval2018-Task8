"""
------------------------------------------------------------
Task2.py
------------------------------------------------------------
Classify sentences relevant to malware.

Run using
  python Task2.py <run>
where <run> is an experiment run number ie. an integer from
1 to 5 inclusive.

This script will automatically run CRF++ and write to
multiple folders. The final results will be stored in 
./Run<run>/scores/. For the normal scores, take the results
from the top half (phrase-level). For the relaxed scores, 
take the results from the second half (token-level).
------------------------------------------------------------
"""

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn import metrics

from os import listdir
from subprocess import call 
import sys

def main():

  if len(sys.argv)==2:
    try:
      run = int(sys.argv[1])
      if run>5 or run<1:
        print "Enter experiment run number (1 to 5)."
        return
    except ValueError:
      print "Enter experiment run number."
      return
  else:
    print "Enter experiment run number."
    return

  models_folder = "./Run"+str(run)+"/models/"
  dev_folder = "./Run"+str(run)+"/dev/"
  results_folder = "./Run"+str(run)+"/results/"
  dev_scores_folder = "./Run"+str(run)+"/dev_scores/"
  scores_folder = "./Run"+str(run)+"/scores/"
  temp_folder = "./Run"+str(run)+"/temp/"
  train = "./Run"+str(run)+"/data/train.data"
  train_t1 = "./Run"+str(run)+"/data/train-t1.data"
  train_t2 = "./Run"+str(run)+"/data/train-t2.data"
  dev = "./Run"+str(run)+"/data/dev.data"
  dev_t1 = "./Run"+str(run)+"/data/dev-t1.data"
  dev_t2 = "./Run"+str(run)+"/data/dev-t2.data"
  test = "./Run"+str(run)+"/data/test.data"
  test_t1 = "./Run"+str(run)+"/data/test-t1.data"
  template = "./CRF_templates/template3a"
  results_t1 = "./Run"+str(run)+"/filtered/results-t1.data"

  c_val_power = [-3,-2,-1,0,1,2,3]

  ### Approach 1

  # Train CRF model for Approach 1
  command = ''
  for cNo,c in enumerate(c_val_power):
    command+="crf_learn -a CRF-L2 -c "+str(10**c)+" "+template+" "+train+" "+models_folder+"model_1_c1e"+str(c)
    if cNo<len(c_val_power)-1: command+=" && "
  call(command, shell=True)

  # Validate CRF model for Approach 1
  command = ''
  for idx,model in enumerate(listdir(models_folder)):
    if '_1_' in model:
      command+="crf_test -m "+models_folder+model+" "+dev+" >> "+dev_folder+"dev_1_"+model+".txt"
      if idx<len(listdir(models_folder))-1: command+=" && "
  call(command, shell=True)

  # Test CRF model for Approach 1
  command = ''
  for idx,model in enumerate(listdir(models_folder)):
    if '_1_' in model:
      command+="crf_test -m "+models_folder+model+" "+test+" >> "+results_folder+"results_1_"+model+".txt"
      if idx<len(listdir(models_folder))-1: command+=" && "
  call(command, shell=True)

  ### Approach 2

  # Train CRF model for Approach 2
  command = ''
  for cNo,c in enumerate(c_val_power):
    command+="crf_learn -a CRF-L2 -c "+str(10**c)+" "+template+" "+train_t2+" "+models_folder+"model_2_c1e"+str(c)
    if cNo<len(c_val_power)-1: command+=" && "
  call(command, shell=True)

  # Validate CRF model for Approach 2
  command = ''
  for idx,model in enumerate(listdir(models_folder)):
    if '_2_' in model:
      command+="crf_test -m "+models_folder+model+" "+dev_t2+" >> "+dev_folder+"dev_2_"+model+".txt"
      if idx<len(listdir(models_folder))-1: command+=" && "
  call(command, shell=True)

  # Train relevance filter, validate and generate filtered file for Approach 2

  trainX_t1,trainY_t1 = generateData_t1(train_t1)
  devX_t1,devY_t1 = generateData_t1(train_t1)
  testX_t1,testY_t1 = generateData_t1(test_t1)

  reg_values = [-3,-2,-1,0,1,2,3]

  f1 = []
  filter_models = []

  for reg in reg_values:
    pipeline = Pipeline([('count_vectorizer',CountVectorizer(ngram_range=(1,2))),('classifier',MultinomialNB(alpha=10.0**reg))])
    pipeline.fit(trainX_t1,trainY_t1)
    expected_dev = devY_t1
    predicted_dev = pipeline.predict(devX_t1)
    filter_models.append(pipeline)
    f1.append(metrics.f1_score(expected_dev, predicted_dev))

  expected = testY_t1
  predicted = filter_models[f1.index(max(f1))].predict(testX_t1)

  predictedRelevantSentences = []
  for predictionNo,prediction in enumerate(predicted):
    if prediction==1: predictedRelevantSentences.append(predictionNo)

  sentences = []
  with open(test,'r') as f:
    sentence = ''
    for line in f:
      if line!='\n': sentence+=line
      else:
        sentences.append(sentence)
        sentence = ''

  with open(results_t1,'w') as f:
    for sentenceNo in predictedRelevantSentences:
      sentence = sentences[sentenceNo]
      f.write(sentence)
      f.write('\n')

  # Test CRF model on filtered file for Approach 2
  command = ''
  for idx,model in enumerate(listdir(models_folder)):
    if '_2_' in model:
      command+="crf_test -m "+models_folder+model+" "+results_t1+" >> "+temp_folder+"results_2tmp_"+model+".txt"
      if idx<len(listdir(models_folder))-1: command+=" && "
  call(command, shell=True)

  # Add back filtered irrelevant sentences

  for filename in listdir(temp_folder):
    if "_2tmp_" in filename:
      predictedIrrelevantSentences = []
      for predictionNo,prediction in enumerate(predicted):
        if prediction==0: predictedIrrelevantSentences.append(predictionNo)

      results_t2_string = ''
      with open(temp_folder+filename,'r') as f:
        for line in f: results_t2_string+=line

      sentences = []
      with open(test,'r') as f:
        sentence = ''
        for line in f:
          if line!='\n':
            line = line.replace(' ','\t')
            sentence+=line[:-1]+"\tO\n"
          else:
            sentences.append(sentence)
            sentence = ''

      with open(results_folder+filename.replace("2tmp","2"),'w') as f:
        for sentenceNo in predictedIrrelevantSentences:
          sentence = sentences[sentenceNo]
          f.write(sentence)
          f.write('\n')
        f.write(results_t2_string)

  # Generate relaxed test results
  for filename in listdir(results_folder):
    if 'results' in filename:
      text = ''
      with open(results_folder+filename,'r') as f:
        for line in f:
          if line!='\n' and (line.split('\t')[3][0]=='B' or line.split('\t')[4][0]=='B'):
            newline = line.split('\t')[0]+'\t'+line.split('\t')[1]+'\t'+line.split('\t')[2]+'\t'+line.split('\t')[3].replace('B','I')+'\t'+line.split('\t')[4].replace('B','I')
          else:
            newline = line
          text+=newline
      with open(results_folder+filename.split('.txt')[0]+'_relaxed.txt','w') as f:
        f.write(text)

  # Score dev results
  command = ''
  for idx,dev in enumerate(listdir(dev_folder)):
    if 'dev' in dev:
      command+="perl conlleval < \""+dev_folder+dev+"\" >> \""+dev_scores_folder+"conlleval-"+dev+"\""
      command+=" && perl conlleval -r < \""+dev_folder+dev+"\" >> \""+dev_scores_folder+"conlleval-"+dev+"\""
      if idx<len(listdir(dev_folder))-1: command+=" && "
  call(command, shell=True)

  # Score test results
  command = ''
  for idx,result in enumerate(listdir(results_folder)):
    if 'results' in result:
      command+="perl conlleval < \""+results_folder+result+"\" >> \""+scores_folder+"conlleval-"+result+"\""
      command+=" && perl conlleval -r < \""+results_folder+result+"\" >> \""+scores_folder+"conlleval-"+result+"\""
      if idx<len(listdir(results_folder))-1: command+=" && "
  call(command, shell=True)

def generateData_t1(dataFile):

  lines = []
  with open(dataFile,'r') as f:
    for line in f: lines.append(line)

  X = []
  Y = []
  sentence = ''

  for line in lines:
    if line!='\n':
      value = int(line.split(' ')[-1][:-1])
      sentence+=line.split(' ')[0]+' '

    else:
      X.append(sentence)
      Y.append(int(value))
      sentence = ''
      value = 'o'

  return X,Y

if __name__ == "__main__":
  main()
