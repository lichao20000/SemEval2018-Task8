# -*- coding: utf-8 -*-
"""
25 Apr 2017
To summarize the scores produced by Task2.py
"""

# Import statements
from __future__ import print_function
import re

def main():
    tok_types = ['Entity', 'Verb', 'Preposition', 'overall']
    results = {True:{'app1':{}, 'app2':{}}, False:{'app1':{}, 'app2':{}}}
    for relaxed in [False, True]:
        for tok_type in tok_types:
            results[relaxed]['app1'][tok_type] = []
            results[relaxed]['app2'][tok_type] = []
    print('        {:^25s} | {:^25s}'.format('Approach 1', 'Approach 2'))
    for run in range(1, 6):
        dev_dir = 'Run{}/dev_scores'.format(run)
        test_dir = 'Run{}/scores'.format(run)
        for app in [1,2]:
            best_score = 0.0
            best_para = None
            for para in [-3, -2, -1, 0, 1, 2, 3]:
                dev_filename = 'conlleval-dev_{}_model_{}_c1e{}.txt'.format(app, app, para)
                dev_filename = '{}/{}'.format(dev_dir, dev_filename)
                scores = read_scores(dev_filename, relaxed=False)
                if scores['overall'][2] > best_score:
                    best_score = scores['overall'][2]
                    best_para = para
            filename = 'conlleval-results_{}_model_{}_c1e{}{}.txt'
            for relaxed in [False, True]:
                if relaxed:
                    test_filename = filename.format(app, app, best_para, '_relaxed')
                else:
                    test_filename = filename.format(app, app, best_para, '')
                test_filename = '{}/{}'.format(test_dir, test_filename)
                scores = read_scores(test_filename, relaxed=relaxed)
                for tok_type, score in scores.items():
                    results[relaxed]['app{}'.format(app)][tok_type].append({'alpha':best_para, 'scores':score})
        print('Experiment {}'.format(run))
        for relaxed in [False, True]:
            print('Relaxed: {}'.format(relaxed))
            for tok_type in tok_types:
                score1 = results[relaxed]['app1'][tok_type][-1]
                score2 = results[relaxed]['app2'][tok_type][-1]
                print('  {0:4s}: {1:2d} {2[0]:5.2f} {2[1]:5.2f} {2[2]:5.2f} {2[4]:4d} | {3:2d} {4[0]:5.2f} {4[1]:5.2f} {4[2]:5.2f} {4[4]:4d}'.format(tok_type[:4], score1['alpha'], score1['scores'], score2['alpha'], score2['scores']))
    overall = {True:{'app1':{}, 'app2':{}}, False:{'app1':{}, 'app2':{}}}
    for relaxed in [False, True]:
        for tok_type in tok_types:
            overall[relaxed]['app1'][tok_type] = [0, 0, 0, 0, 0, 0]
            overall[relaxed]['app2'][tok_type] = [0, 0, 0, 0, 0, 0]
    for relaxed in [False, True]:
        for app in ['app1', 'app2']:
            for tok_type in tok_types:
                overall_result = overall[relaxed][app][tok_type]
                for result in results[relaxed][app][tok_type]:
                    for i in range(6):
                        overall_result[i] += result['scores'][i]
                overall_result[0] = 100.0*overall_result[5]/overall_result[3]
                overall_result[1] = 100.0*overall_result[5]/overall_result[4]
                overall_result[2] = harm_mean(overall_result[0], overall_result[1])
    print('Overall result')
    print('        {:^22s} | {:^22s}'.format('Approach 1', 'Approach 2'))
    for relaxed in [False, True]:
        print('Relaxed: {}'.format(relaxed))
        for tok_type in tok_types:
            score1 = overall[relaxed]['app1'][tok_type]
            score2 = overall[relaxed]['app2'][tok_type]
            print('  {0:4s}: {1[0]:5.2f} {1[1]:5.2f} {1[2]:5.2f} {1[4]:4d} | {2[0]:5.2f} {2[1]:5.2f} {2[2]:5.2f} {2[4]:4d}'.format(tok_type[:4], score1, score2))


def harm_mean(a, b):
    if a*b == 0:
        return 0.0
    else:
        return 2*a*b/(a+b)

def read_scores(filename, relaxed=False):
    results = {}
    supports = []
    accuracies = []
    with open(filename, 'r') as infile:
        for line in infile:
            line = line.strip('\n.')
            if line.startswith('processed'):
                tokens = line.split(' ')
                pred, gold, cor = int(tokens[7]), int(tokens[4]), int(tokens[10])
                supports.append((pred, gold, cor))
            elif line.startswith('accuracy'):
                tokens = re.split('[ \t%;:]+', line)
                accuracies.append((float(tokens[3]), float(tokens[5]), float(tokens[7])))
            else:
                tokens = re.split('[ \t%;:]+', line)
                p, r, f, pred = float(tokens[3]), float(tokens[5]), float(tokens[7]), int(tokens[8])
                cor = int(round(pred*p/100.0))
                if r == 0:
                    gold = 0
                else:
                    gold = int(round(100.0*cor/r))
                if relaxed:
                    if tokens[1].startswith('I-'):
                        results[tokens[1][2:]] = (p, r, f, pred, gold, cor)
                else:
                    if tokens[1][1] != '-':
                        results[tokens[1]] = (p, r, f, pred, gold, cor)
    if relaxed:
        results['overall'] = (accuracies[1][0], accuracies[1][1], accuracies[1][2], supports[1][0], supports[1][1], supports[1][2])
    else:
        results['overall'] = (accuracies[0][0], accuracies[0][1], accuracies[0][2], supports[0][0], supports[0][1], supports[0][2])
    return results

if __name__ == '__main__':
    main()

