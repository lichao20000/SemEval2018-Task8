"""
------------------------------------------------------------
Task1.py
------------------------------------------------------------
Classify sentences relevant to malware.

Run using
  python Task1.py <micro_average:true|false>
------------------------------------------------------------
"""

from __future__ import print_function
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn import metrics

from os import listdir
import sys

def main():

  micro_average = False
  if len(sys.argv) > 1:
      micro_average = bool(sys.argv[1])

  tokenizedFolder = "../../data/tokenized/"
  results = {}
  results['svm'] = []
  results['nb'] = []

  print('         {:^22s} |    {:^22s}'.format('SVM', 'Naive Bayes'))
  print('         {:^5s} {:^5s} {:^5s} {:^4s} |    {:^5s} {:^5s} {:^5s} {:^4s}'.format('P', 'R', 'F', 'n', 'P', 'R', 'F', 'n'))
  for run in range(1, 6):
      trainingSet,devSet,testSet,SVC_para,NB_para = getRunData(run)

      trainX,trainY = generateData(trainingSet,tokenizedFolder)
      devX,devY = generateData(devSet,tokenizedFolder)
      testX,testY = generateData(testSet,tokenizedFolder)

      cv = CountVectorizer(ngram_range=(1,2))
      trainX_counted = cv.fit_transform(trainX)
      devX_counted = cv.transform(devX)
      testX_counted = cv.transform(testX)

      # LinearSVC
      # pipeline = Pipeline([('count_vectorizer',CountVectorizer(ngram_range=(1,2))),('classifier',LinearSVC(C=10**(SVC_para)))])
      # pipeline.fit(trainX,trainY)

      best_para = None
      best_model = None
      best_f1 = 0
      print()
      print('\tFinding best parameter for SVC')
      print('\t', end='')
      for SVC_para in [-3, -2, -1, 0, 1, 2, 3]:
        cls = LinearSVC(C=10**SVC_para)
        cls.fit(trainX_counted, trainY)
        dev_pred = cls.predict(devX_counted)
        score = metrics.precision_recall_fscore_support(devY, dev_pred)
        print('{:.2f}'.format(100*score[2][1]), end=' ')
        if score[2][1] > best_f1:
            best_f1 = score[2][1]
            best_para = SVC_para
            best_model = cls
      print('The best is: {}'.format(best_para))

      expected = testY
      predicted = best_model.predict(testX_counted)

      # print("Classification report for classifier %s:\n%s\n" % (pipeline, metrics.classification_report(expected, predicted,digits=4)))
      result = list(list(zip(*metrics.precision_recall_fscore_support(expected, predicted)))[1])
      result.append(int(round(result[1]*result[3])))
      result[3:3] = [int(round(result[4]/result[0]))]
      result[0] *= 100
      result[1] *= 100
      result[2] *= 100
      results['svm'].append({'C':best_para, 'scores':result})

      # MultinomialNB

      # pipeline = Pipeline([('count_vectorizer',CountVectorizer(ngram_range=(1,2))),('classifier',MultinomialNB(alpha=10**(NB_para)))])
      # pipeline.fit(trainX,trainY)

      best_para = None
      best_model = None
      best_f1 = 0
      print('\tFinding best parameter for NB')
      print('\t', end='')
      for NB_para in [-3, -2, -1, 0, 1, 2, 3]:
        cls = MultinomialNB(alpha=10**NB_para)
        cls.fit(trainX_counted, trainY)
        dev_pred = cls.predict(devX_counted)
        score = metrics.precision_recall_fscore_support(devY, dev_pred)
        print('{:.2f}'.format(100*score[2][1]), end=' ')
        if score[2][1] > best_f1:
            best_f1 = score[2][1]
            best_para = NB_para
            best_model = cls
      print('The best is: {}'.format(best_para))
      print()


      expected = testY
      predicted = best_model.predict(testX_counted)

      # print("Classification report for classifier %s:\n%s\n" %(pipeline, metrics.classification_report(expected, predicted,digits=4)))
      result = list(list(zip(*metrics.precision_recall_fscore_support(expected, predicted)))[1])
      result.append(int(round(result[1]*result[3])))
      result[3:3] = [int(round(result[4]/result[0]))]
      result[0] *= 100
      result[1] *= 100
      result[2] *= 100
      results['nb'].append({'C':best_para, 'scores':result})

      print('Exp{2:d}: {3:2d} {0[0]:5.2f} {0[1]:5.2f} {0[2]:5.2f} {0[4]:4d} | {4:2d} {1[0]:5.2f} {1[1]:5.2f} {1[2]:5.2f} {1[4]:4d}'.format(
            results['svm'][-1]['scores'], results['nb'][-1]['scores'], run, results['svm'][-1]['C'], results['nb'][-1]['C']))
  overall = {'svm':[0, 0, 0, 0, 0, 0], 'nb':[0, 0, 0, 0, 0, 0]}
  for model in ['svm', 'nb']:
    for result in results[model]:
      for i in range(6):
        overall[model][i] += result['scores'][i]
  for model in ['svm', 'nb']:
    if micro_average:
      overall[model][0] = 100.0*overall[model][5]/overall[model][3]
      overall[model][1] = 100.0*overall[model][5]/overall[model][4]
      overall[model][2] = harm_mean(overall[model][0], overall[model][1])
    else:
      for i in range(3):
        overall[model][i] /= 5
  print('Overall Results')
  print('{:^22s} | {:^22s}'.format('SVM', 'Naive Bayes'))
  print('{:^5s} {:^5s} {:^5s} {:^4s} | {:^5s} {:^5s} {:^5s} {:^4s}'.format('P', 'R', 'F', 'n', 'P', 'R', 'F', 'n'))
  print('{0[0]:5.2f} {0[1]:5.2f} {0[2]:5.2f} {0[3]:4d} | {1[0]:5.2f} {1[1]:5.2f} {1[2]:5.2f} {1[3]:4d}'.format(
        overall['svm'], overall['nb']))
  print()

def harm_mean(a, b):
    if a*b == 0.0:
        return 0.0
    return 2*a*b/(a+b)

def getRunData(runNo):

  datasets = {}
  all_para_same = False

  trainingSet1 = [0,1,3,4,6,7,10,11,12,13,14,16,17,18,23,24,28,29,30,31,33,36,38]
  devSet1 = [27,32,15,25,5,8,22,21]
  testSet1 = [35,9,20,34,37,26,19,2]
  SVC_para_1 = -1
  NB_para_1 = -1

  trainingSet2 = [1,2,5,6,8,13,15,16,17,18,19,20,22,23,25,28,30,31,34,35,36,37,38]
  devSet2 = [33,7,14,26,11,4,32,12]
  testSet2 = [21,24,0,29,27,3,9,10]
  SVC_para_2 = -1
  NB_para_2 = -1

  trainingSet3 = [0,1,3,5,7,9,10,11,12,13,16,18,19,21,23,24,25,26,29,32,35,36,38]
  devSet3 = [20,17,4,31,34,14,2,37]
  testSet3 = [27,15,6,22,33,8,30,28]
  SVC_para_3 = -2
  NB_para_3 = 0
  if all_para_same:
    SVC_para_3 = -1
    NB_para_3 = -1

  trainingSet4 = [2,3,5,6,8,9,10,11,13,14,16,17,19,20,22,23,27,29,30,31,32,34,38]
  devSet4 = [1,4,35,26,33,25,12,0]
  testSet4 = [18,15,36,37,7,28,21,24]
  SVC_para_4 = 1
  NB_para_4 = 0
  if all_para_same:
    SVC_para_4 = -1
    NB_para_4 = -1

  trainingSet5 = [1,2,7,8,11,12,13,14,15,16,19,20,22,23,24,26,27,29,31,32,33,36,38]
  devSet5 = [4,30,17,18,0,6,37,35]
  testSet5 = [21,3,28,25,34,10,5,9]
  SVC_para_5 = -1
  NB_para_5 = 0
  if all_para_same:
    SVC_para_5 = -1
    NB_para_5 = -1

  datasets[1] = (trainingSet1,devSet1,testSet1,SVC_para_1,NB_para_1)
  datasets[2] = (trainingSet2,devSet2,testSet2,SVC_para_2,NB_para_2)
  datasets[3] = (trainingSet3,devSet3,testSet3,SVC_para_3,NB_para_3)
  datasets[4] = (trainingSet4,devSet4,testSet4,SVC_para_4,NB_para_4)
  datasets[5] = (trainingSet5,devSet5,testSet5,SVC_para_5,NB_para_5)

  return datasets[runNo]

def generateData(fileIndex,dataFolder):

  X = []
  Y = []
  sentence = ''
  relevance = 0

  for i,fileName in enumerate(listdir(dataFolder)):
    if i in fileIndex:

      with open(dataFolder+fileName,'r') as f:
        for line in f:
          if line=='\n':
            if sentence!='':
              X.append(sentence)
              Y.append(relevance)
            sentence = ''
            relevance = 0
          else:
            if sentence=='': sentence = line.split(' ')[0]
            else: sentence+=' '+line.split(' ')[0]
            if line[:-1].split(' ')[-1]!='O': relevance = 1

  return X,Y


if __name__ == "__main__":
  main()
