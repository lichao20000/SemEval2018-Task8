"""
------------------------------------------------------------
Task4.py
------------------------------------------------------------

Predict attribute labels for the different attribute
categories.

Run using
    python Task4.py <run> <attribute-type>
where <run> is an experiment run number ie. an integer from
1 to 5 inclusive and <attribute-type> is the uppercase
initial of the desired attribute category ('C','A','S','T').
------------------------------------------------------------
"""

from __future__ import print_function
from os import listdir

import sys
reload(sys)
sys.setdefaultencoding('utf-8')

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn import metrics

import numpy as np

def main():

    attributeTypes = {'C':'capability/','A':'actionName/','S':'strategicObjectives/','T':'tacticalObjectives/'}
    attributeSize = {'C':20,'A':211,'S':65,'T':148}

    # if len(sys.argv)==3:
    #     try:
    #         run = int(sys.argv[1])
    #         if run>5 or run<1:
    #             print("Enter experiment run number (1 to 5).")
    #             return
    #     except ValueError:
    #         print("Enter experiment run number (1 to 5).")
    #         return
    #     if len(sys.argv[2])==1:
    #         try:
    #             attributeType = attributeTypes[sys.argv[2]]
    #         except IndexError:
    #             print("Enter attribute-type ('C','A','S' or 'T').")
    #             return
    #     else:
    #         print("Enter attribute-type ('C','A','S' or 'T').")
    #         return
    # else:
    #     print("Enter experiment run number and attribute-type.")
    #     return

    print('         {:^22s} |    {:^22s}'.format('SVM', 'Naive Bayes'))
    print('         {:^5s} {:^5s} {:^5s} {:^4s} |    {:^5s} {:^5s} {:^5s} {:^4s}'.format('P', 'R', 'F', 'n', 'P', 'R', 'F', 'n'))
    for att_key, attributeType in attributeTypes.items():
        results = {'svm':[], 'nb':[]}
        print('For attribute: {}'.format(attributeType))
        for run in range(1, 6):
            dataFolder = "attributes/"+attributeType

            documents = []

            lines = []
            for row in listdir(dataFolder):
                if row!=".DS_Store":
                    documents.append(row)
                    lines.append([])
                    with open(dataFolder+row,'r') as f:
                        for line in f: lines[-1].append(line)

            allsentences = []
            allsentences_i = []

            sentence = ''

            for i,document in enumerate(lines):
                allsentences.append([])
                allsentences_i.append(i)
                for line in document:
                    if line!='\n':
                        if line.split(' ')[1][:-1]=='O': value = -1
                        else: value = int(line.split(' ')[1][:-1])
                        sentence+=line.split(' ')[0]+' '

                    else:
                        allsentences[-1].append([sentence,int(value)])
                        sentence = ''
                        value = 'o'

            trainingSet,devSet,testSet = getRunData(run)

            trainX,trainY,trainDoc,allsentences_i = assignSentences(allsentences,allsentences_i,documents,trainingSet)
            devX,devY,devDoc,allsentences_i = assignSentences(allsentences,allsentences_i,documents,devSet)
            testX,testY,testDoc,allsentences_i = assignSentences(allsentences,allsentences_i,documents,testSet)

            alphas = np.array(range(-4, 5))

            # devResults = []
            # classifiers = []
            
            cv = CountVectorizer(ngram_range=(1,2))
            trainX_counted = cv.fit_transform(trainX)
            devX_counted = cv.transform(devX)
            testX_counted = cv.transform(testX)

            best_alpha = None
            best_model = None
            best_score = 0.0
            for alpha in alphas:

                # pipeline = Pipeline([('count_vectorizer',CountVectorizer(ngram_range=(1,2))),('classifier',MultinomialNB(alpha=10.0**alpha))])
                # pipeline.fit(trainX,trainY)
                # classifiers.append(pipeline)
                cls = MultinomialNB(alpha=10.0**alpha)
                cls.fit(trainX_counted, trainY)

                predicted_dev = cls.predict(devX_counted)
                expected_dev = devY
                score = metrics.precision_recall_fscore_support(expected_dev, predicted_dev,labels=range(attributeSize[att_key]), average='micro')
                if score[2] > best_score:
                    best_score = score[2]
                    best_alpha = alpha
                    best_model = cls
                # devResults.append(metrics.precision_recall_fscore_support(expected_dev, predicted_dev,labels=range(attributeSize[sys.argv[2]]), average='micro'))

            expected = testY
            predicted = best_model.predict(testX_counted)
            # predicted = classifiers[devResults.index(max(devResults))].predict(testX)
            # print(metrics.classification_report(expected, predicted, digits=4))
            # print(metrics.precision_recall_fscore_support(expected, predicted,labels = range(attributeSize[att_key]),average='micro'))
            result = metrics.precision_recall_fscore_support(expected, predicted, labels=range(attributeSize[att_key]), average='micro')
            gold = sum(metrics.precision_recall_fscore_support(expected, predicted, labels=range(attributeSize[att_key]))[3])
            correct = int(round(gold*result[1]))
            predicted = int(round(correct/result[0]))
            result = (100*result[0], 100*result[1], 100*result[2], predicted, gold, correct)
            results['nb'].append({'alpha':best_alpha, 'scores':result})

            # devResults = []
            # classifiers = []

            best_alpha = None
            best_model = None
            best_score = 0.0
            for alpha in alphas:

                # pipeline = Pipeline([('count_vectorizer',CountVectorizer(ngram_range=(1,2))),('classifier',LinearSVC(C=10.0**alpha))])
                # pipeline.fit(trainX,trainY)
                # classifiers.append(pipeline)
                cls = LinearSVC(C=10.0**alpha)
                cls.fit(trainX_counted, trainY)

                expected_dev = devY
                # predicted_dev = pipeline.predict(devX)
                predicted_dev = cls.predict(devX_counted)
                expected_dev = devY
                score = metrics.precision_recall_fscore_support(expected_dev, predicted_dev,labels=range(attributeSize[att_key]), average='micro')
                if score[2] > best_score:
                    best_score = score[2]
                    best_alpha = alpha
                    best_model = cls

                # devResults.append(metrics.f1_score(expected_dev, predicted_dev,labels = range(attributeSize[sys.argv[2]]), average='micro'))

            expected = testY
            predicted = best_model.predict(testX_counted)
            # predicted = classifiers[devResults.index(max(devResults))].predict(testX)
            # print(metrics.classification_report(expected, predicted, digits=4))
            # print(metrics.precision_recall_fscore_support(expected, predicted,labels = range(attributeSize[att_key]),average='micro'))
            result = metrics.precision_recall_fscore_support(expected, predicted, labels=range(attributeSize[att_key]), average='micro')
            gold = sum(metrics.precision_recall_fscore_support(expected, predicted, labels=range(attributeSize[att_key]))[3])
            correct = int(round(gold*result[1]))
            predicted = int(round(correct/result[0]))
            result = (100*result[0], 100*result[1], 100*result[2], predicted, gold, correct)
            results['svm'].append({'alpha':best_alpha, 'scores':result})
            print('Exp{2:d}: {3:2d} {0[0]:5.2f} {0[1]:5.2f} {0[2]:5.2f} {0[4]:4d} | {4:2d} {1[0]:5.2f} {1[1]:5.2f} {1[2]:5.2f} {1[4]:4d}'.format(results['svm'][-1]['scores'], results['nb'][-1]['scores'], run, results['svm'][-1]['alpha'], results['nb'][-1]['alpha']))
        overall = {'svm':[0, 0, 0, 0, 0, 0], 'nb':[0, 0, 0, 0, 0, 0]}
        micro_average = True
        for model in ['svm', 'nb']:
            for result in results[model]:
                for i in range(6):
                    overall[model][i] += result['scores'][i]
        for model in ['svm', 'nb']:
            if micro_average:
                overall[model][0] = 100.0*overall[model][5]/overall[model][3]
                overall[model][1] = 100.0*overall[model][5]/overall[model][4]
                overall[model][2] = harm_mean(overall[model][0], overall[model][1])
            else:
                for i in range(3):
                    overall[model][i] /= 5
        print('Overall Results')
        print('{:^22s} | {:^22s}'.format('SVM', 'Naive Bayes'))
        print('{:^5s} {:^5s} {:^5s} {:^4s} | {:^5s} {:^5s} {:^5s} {:^4s}'.format('P', 'R', 'F', 'n', 'P', 'R', 'F', 'n'))
        print('{0[0]:5.2f} {0[1]:5.2f} {0[2]:5.2f} {0[4]:4d} | {1[0]:5.2f} {1[1]:5.2f} {1[2]:5.2f} {1[4]:4d}'.format(overall['svm'], overall['nb']))
        print()

def harm_mean(a, b):
    if a*b == 0.0:
        return 0.0
    else:
        return 2*a*b/(a+b)

def getRunData(run):

    datasets = {}

    trainingSet1 = [7, 9, 29, 8, 10, 21, 18, 35, 11, 14, 6, 26, 13, 36, 38, 15, 2, 22, 3, 34, 0, 20, 32]
    devSet1 = [24, 19, 23, 12, 37, 31, 16, 1]
    testSet1 = [33, 25, 17, 30, 27, 28, 5, 4]

    trainingSet2 = [17, 14, 37, 16, 28, 4, 2, 35, 8, 25, 10, 33, 12, 6, 32, 1, 21, 3, 29, 38, 31, 30, 20]
    devSet2 = [23, 34, 22, 36, 11, 26, 13, 5]
    testSet2 = [27, 0, 24, 18, 19, 15, 7, 9]

    trainingSet3 = [32, 7, 22, 13, 2, 25, 28, 30, 16, 14, 23, 3, 24, 6, 8, 35, 37, 17, 1, 5, 12, 11, 4]
    devSet3 = [26, 0, 33, 18, 19, 15, 9, 31]
    testSet3 = [27, 34, 38, 10, 29, 36, 21, 20]

    trainingSet4 = [15, 28, 17, 27, 33, 10, 13, 36, 0, 16, 14, 35, 1, 2, 30, 18, 26, 6, 11, 25, 19, 4, 34]
    devSet4 = [31, 21, 9, 32, 3, 37, 23, 7]
    testSet4 = [12, 5, 38, 8, 29, 24, 22, 20]

    trainingSet5 = [30, 27, 0, 13, 8, 38, 1, 34, 26, 16, 3, 24, 10, 19, 31, 5, 23, 14, 18, 32, 17, 2, 37]
    devSet5 = [36, 28, 6, 12, 15, 33, 7, 29]
    testSet5 = [35, 21, 25, 11, 20, 4, 22, 9]

    datasets[1] = (trainingSet1,devSet1,testSet1)
    datasets[2] = (trainingSet2,devSet2,testSet2)
    datasets[3] = (trainingSet3,devSet3,testSet3)
    datasets[4] = (trainingSet4,devSet4,testSet4)
    datasets[5] = (trainingSet5,devSet5,testSet5)

    return datasets[run]

def assignSentences(allsentences,allsentences_i,documents,index):

    X = []
    Y = []
    doc = []

    for choice in index:

        allsentences_i.pop(allsentences_i.index(choice))
        doc.append(choice)
        for sentence in allsentences[choice]:
            X.append(sentence[0])
            Y.append(sentence[1])

    return X,Y,doc,allsentences_i

if __name__ == "__main__":
    main()
